{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b670d70-8f61-4350-be6c-38a69edefb5a",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e37ed58-b15c-4dd0-8838-cc06394517b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T12:48:11.556722Z",
     "iopub.status.busy": "2023-01-15T12:48:11.556425Z",
     "iopub.status.idle": "2023-01-15T12:48:11.567269Z",
     "shell.execute_reply": "2023-01-15T12:48:11.566462Z",
     "shell.execute_reply.started": "2023-01-15T12:48:11.556691Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch import optim, nn\n",
    "from PIL import  Image\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import  WeightedRandomSampler\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3757cdfa-2be9-4d8d-b9b3-6c0064f29c83",
   "metadata": {},
   "source": [
    "# set_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9fc3860-d8bf-4ec4-a05d-a7167f0d9409",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T10:06:12.466444Z",
     "iopub.status.busy": "2023-01-15T10:06:12.465971Z",
     "iopub.status.idle": "2023-01-15T10:06:12.526438Z",
     "shell.execute_reply": "2023-01-15T10:06:12.525485Z",
     "shell.execute_reply.started": "2023-01-15T10:06:12.466382Z"
    }
   },
   "outputs": [],
   "source": [
    "inchan = 7 # set 1 to 10\n",
    "CT_type = 'P' # 'P' or 'CE'\n",
    "seed = 1 # random seed\n",
    "gpu = \"cuda:0\"\n",
    "n_class = 2 # classes number\n",
    "save_state = 1 # save the state [0 or 1]\n",
    "\n",
    "model_name = 'Xception'\n",
    "opt_name = 'adam'\n",
    "epochs = 50 # epoch number\n",
    "size_in = 171\n",
    "bs  = 120 # training_batchsize\n",
    "bs2 = 60 # testing_batchsize\n",
    "\n",
    "pre_dir_name = ('total_save/' # total\n",
    "                +'CT_type:'+str(CT_type)\n",
    "                +'-inchan:'+str(inchan)\n",
    "                +'-n_classes:'+str(n_class)\n",
    "                +'-seed:'+str(seed))\n",
    "\n",
    "path_CV = 'data/Center1_CV'\n",
    "path_In_Val = 'data/Center1_hold_out'\n",
    "path_Ex_Val = 'data/Center2'\n",
    "\n",
    "total_ID_list = os.listdir(path_CV)\n",
    "In_Val_ID_list = os.listdir(path_In_Val)\n",
    "Ex_Val_ID_list = os.listdir(path_Ex_Val)\n",
    "\n",
    "dev = t.device(gpu) if t.cuda.is_available() else t.device(\"cpu\")\n",
    "\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1722355a-e5a8-48b2-ad78-66a978581104",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e98cbb2-0f03-42e6-991c-fbcce53a8637",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T09:50:30.292071Z",
     "iopub.status.busy": "2023-01-15T09:50:30.291653Z",
     "iopub.status.idle": "2023-01-15T09:50:30.309971Z",
     "shell.execute_reply": "2023-01-15T09:50:30.308972Z",
     "shell.execute_reply.started": "2023-01-15T09:50:30.292016Z"
    }
   },
   "outputs": [],
   "source": [
    "std_list = list()\n",
    "for i in range(inchan):\n",
    "    std_list.append(0.5)\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(), \n",
    "    T.Normalize(mean=std_list, std=std_list) ])\n",
    "\n",
    "mirror = T.Compose([\n",
    "    T.ToTensor(), \n",
    "    T.Normalize(mean=std_list, std=std_list), \n",
    "    T.RandomHorizontalFlip(p=1)])\n",
    "\n",
    "class CTdataset(Dataset):\n",
    "    def __init__(self, imgs_list,transform = None, size_in = None):\n",
    "        self.imgs_path = imgs_list\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_nX = self.imgs_path[index]\n",
    "        img_X = img_nX[0]\n",
    "        if 'KF' in img_X:\n",
    "            label = 0\n",
    "        else:\n",
    "            label = 1\n",
    "\n",
    "        image_data = np.zeros([size_in, size_in,inchan])\n",
    "        for i in range(inchan):\n",
    "            pil_img_n = Image.open(img_nX[i]).convert('L').resize((size_in, size_in),Image.ANTIALIAS)\n",
    "            image_data[:,:,i] = pil_img_n\n",
    "        image_data = image_data.astype(np.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(image_data)\n",
    "        else:\n",
    "            data = image_data\n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs_path)\n",
    "    \n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    out = model(xb)\n",
    "    preds = t.argmax(out,dim=1)\n",
    "    loss = loss_func(out, yb)\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), preds\n",
    "\n",
    "\n",
    "def get_data(train_ds,bs,sampler=None):\n",
    "    if sampler:\n",
    "        t1 = DataLoader(train_ds, batch_size=bs,sampler=sampler)\n",
    "    else:\n",
    "        t1 = DataLoader(train_ds, batch_size=bs)\n",
    "    return t1\n",
    "\n",
    "def get_ID_root_label(ID,path):\n",
    "    ID_root_list = list()\n",
    "    path_ID = os.path.join(path,ID)\n",
    "    ID_P_list = sorted(glob.glob(path_ID+'/1.0/*.jpg'))\n",
    "    ID_CE_list = sorted(glob.glob(path_ID+'/1.0CE/*.jpg'))\n",
    "    if CT_type == 'P':\n",
    "        ID_X_list = ID_P_list\n",
    "    if CT_type == 'CE':\n",
    "        ID_X_list = ID_CE_list\n",
    "\n",
    "    for j in range(len(ID_X_list)-inchan+1):                    \n",
    "        trans_list = list()\n",
    "        for k in range(inchan):\n",
    "            trans_list.append(ID_X_list[j+k])\n",
    "        ID_root_list.append(trans_list)\n",
    "    \n",
    "    if 'KF' in ID_X_list[0]:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return(ID_root_list, label)\n",
    "\n",
    "def Find_Optimal_Cutoff(TPR, FPR, Thresholds):\n",
    "    y = TPR - FPR\n",
    "    Youden_index = np.argmax(y) \n",
    "    optimal_threshold = Thresholds[Youden_index]\n",
    "    point = [FPR[Youden_index], TPR[Youden_index]]\n",
    "    return optimal_threshold, point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cb17c8-38aa-41ef-ab53-4a4b469af50d",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ef11ba8-3044-4e33-84e3-9c8f793befa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T09:50:31.671733Z",
     "iopub.status.busy": "2023-01-15T09:50:31.671321Z",
     "iopub.status.idle": "2023-01-15T09:50:31.694525Z",
     "shell.execute_reply": "2023-01-15T09:50:31.693483Z",
     "shell.execute_reply.started": "2023-01-15T09:50:31.671677Z"
    }
   },
   "outputs": [],
   "source": [
    "class depthwise_separable_conv(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size, padding, bias=False):\n",
    "        super(depthwise_separable_conv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, padding=padding, groups=nin, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)                                               \n",
    "        return out\n",
    "    \n",
    "class Xception(nn.Module):\n",
    "    def __init__(self,num_classes= None ,inchan = None, p_drop = None):\n",
    "        super(Xception, self).__init__()\n",
    "        \n",
    "        self.p_drop = p_drop\n",
    "        \n",
    "        # Entry Flow\n",
    "        self.entry_flow_1 = nn.Sequential(\n",
    "            nn.Conv2d(inchan, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.entry_flow_2 = nn.Sequential(\n",
    "            depthwise_separable_conv(64, 128, 3, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            depthwise_separable_conv(128, 128, 3, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.entry_flow_2_residual = nn.Conv2d(64, 128, kernel_size=1, stride=2, padding=0)\n",
    "        \n",
    "        self.entry_flow_3 = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(128, 256, 3, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(256, 256, 3, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.entry_flow_3_residual = nn.Conv2d(128, 256, kernel_size=1, stride=2, padding=0)\n",
    "        \n",
    "        self.entry_flow_4 = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(256, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.entry_flow_4_residual = nn.Conv2d(256, 728, kernel_size=1, stride=2, padding=0)\n",
    "        \n",
    "        # Middle Flow\n",
    "        self.middle_flow = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728)\n",
    "        )\n",
    "        \n",
    "        # Exit Flow\n",
    "        self.exit_flow_1 = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 1024, 3, 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.exit_flow_1_residual = nn.Conv2d(728, 1024, kernel_size=1, stride=2, padding=0)\n",
    "        self.exit_flow_2 = nn.Sequential(\n",
    "            depthwise_separable_conv(1024, 1536, 3, 1),\n",
    "            nn.BatchNorm2d(1536),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            depthwise_separable_conv(1536, 2048, 3, 1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(2048, 1024)\n",
    "        self.linear2 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        entry_out1 = self.entry_flow_1(x)\n",
    "        entry_out2 = self.entry_flow_2(entry_out1) + self.entry_flow_2_residual(entry_out1)\n",
    "        entry_out3 = self.entry_flow_3(entry_out2) + self.entry_flow_3_residual(entry_out2)\n",
    "        entry_out = self.entry_flow_4(entry_out3) + self.entry_flow_4_residual(entry_out3)\n",
    "        \n",
    "        middle_out = self.middle_flow(entry_out) + entry_out\n",
    "\n",
    "        exit_out1 = self.exit_flow_1(middle_out) + self.exit_flow_1_residual(middle_out)\n",
    "        exit_out2 = self.exit_flow_2(exit_out1)\n",
    "\n",
    "        exit_avg_pool = F.adaptive_avg_pool2d(exit_out2, (1, 1))     \n",
    "        exit_avg_pool_flat = exit_avg_pool.view(exit_avg_pool.size(0), -1)\n",
    "\n",
    "        exit_avg_pool_flat = nn.Dropout(p=self.p_drop)(exit_avg_pool_flat)\n",
    "        output = self.linear1(exit_avg_pool_flat)\n",
    "        output = self.linear2(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def xception(x1,x2,x3):\n",
    "    return Xception(num_classes= x1 ,inchan = x2, p_drop = x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049d1c3-0455-424f-9d32-7caf15045790",
   "metadata": {},
   "source": [
    "# fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f1007f0-9261-4e30-ae47-710f91801dc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T09:50:33.759191Z",
     "iopub.status.busy": "2023-01-15T09:50:33.758768Z",
     "iopub.status.idle": "2023-01-15T09:50:33.775768Z",
     "shell.execute_reply": "2023-01-15T09:50:33.774765Z",
     "shell.execute_reply.started": "2023-01-15T09:50:33.759134Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, test_dl):\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_n.append(epoch)\n",
    "        \n",
    "        model.train()\n",
    "        train_acc = list()\n",
    "        train_loss = list()\n",
    "        for xb, yb in train_dl:\n",
    "            xb = xb.to(dev)\n",
    "            yb = yb.to(dev)\n",
    "            tbs_loss, tbs_preds = loss_batch(model, loss_func, xb, yb, opt)\n",
    "            tbs_acc = (tbs_preds == yb).float().mean()\n",
    "            train_loss.append(tbs_loss)\n",
    "            train_acc.append(tbs_acc.tolist())\n",
    "        tr_loss= np.array(train_loss).mean()\n",
    "        tr_acc = np.array(train_acc).mean()\n",
    "        \n",
    "        print('epoch:', epoch, \", trian_loss:\",tr_loss,', trian_acc:',tr_acc )\n",
    "        with open(file_record , \"a\") as f:\n",
    "            f.write('epoch:'+str(epoch)+\", trian_loss:\"+str(tr_loss)+', trian_acc:'+str(tr_acc)+'\\n')\n",
    "        \n",
    "        loss_training.append(tr_loss)\n",
    "        acc_training.append(tr_acc)\n",
    "        \n",
    "        if save_state == 1:\n",
    "            t.save(model.state_dict(), state_name)\n",
    "        \n",
    "        model.eval()\n",
    "        with t.no_grad(): \n",
    "            test_acc = list()\n",
    "            test_loss = list()\n",
    "            for xb, yb in test_dl:\n",
    "                xb = xb.to(dev)\n",
    "                yb = yb.to(dev)\n",
    "                tebs_loss, tebs_preds = loss_batch(model, loss_func, xb, yb)\n",
    "                tebs_acc = (tebs_preds == yb).float().mean()\n",
    "                test_loss.append(tebs_loss)\n",
    "                test_acc.append(tebs_acc.tolist())\n",
    "            \n",
    "            te_loss = np.array(test_loss).mean()\n",
    "            te_acc = np.array(test_acc).mean()\n",
    "            \n",
    "            now_acc = te_acc\n",
    "            \n",
    "            print(\"test_loss:\", te_loss ,\", test_acc:\", now_acc )\n",
    "            with open(file_record , \"a\") as f:\n",
    "                f.write(\"test_loss:\"+str(te_loss)+\", test_acc:\"+str(now_acc)+'\\n' )\n",
    "            \n",
    "            loss_testing.append(te_loss)\n",
    "            acc_testing.append(te_acc)\n",
    "            \n",
    "            if epoch == 0 and save_state == 1: \n",
    "                if save_state == 1:\n",
    "                    t.save(model.state_dict(), best_acc_name)\n",
    "                best_acc = now_acc\n",
    "                best_acc_epoch = epoch\n",
    "            if now_acc > best_acc and epoch > 0 and save_state == 1:\n",
    "                if save_state == 1:\n",
    "                    t.save(model.state_dict(), best_acc_name)\n",
    "                best_acc = now_acc\n",
    "                best_acc_epoch = epoch\n",
    "\n",
    "    print('best_acc_testing:',max(acc_testing),', best_epoch:',best_acc_epoch)\n",
    "    \n",
    "    with open(file_record , \"a\") as f:\n",
    "        f.write('best_epoch:'+str(best_acc_epoch)+'\\n'+'best_acc_testing:'+str(max(acc_testing))+'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f811d37-042a-4140-8d31-12922d388c26",
   "metadata": {},
   "source": [
    "# start_total_traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eba1a4b-8039-4c1f-9e84-d607f50ddf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = (pre_dir_name + '/' + model_name +'-'+ opt_name)          \n",
    "file_record = path_save+'/record-{0}-{1}.txt'.format(model_name, opt_name)\n",
    "csv_name_path = path_save+ '/record-{0}-{1}-lines.csv'.format(model_name, opt_name)\n",
    "state_name = path_save+ '/'+'state-{0}-{1}-done.pkl'.format(model_name, opt_name)\n",
    "best_acc_name = path_save+ '/'+'best_acc-{0}-{1}-done.pkl'.format(model_name, opt_name)\n",
    "\n",
    "print('path_save:',path_save)\n",
    "print('file_record:',file_record)\n",
    "print('csv_name_path:',csv_name_path)\n",
    "\n",
    "if save_state == 1:\n",
    "    if not os.path.exists(path_save):\n",
    "        os.makedirs(path_save)\n",
    "    else:\n",
    "        print('exist')\n",
    "\n",
    "train_imgs_list = list()\n",
    "test_imgs_list = list()\n",
    "\n",
    "for ID in total_ID_list:\n",
    "    path_ID = os.path.join(path_CV,ID)\n",
    "    ID_P_list = sorted(glob.glob(path_ID+'/1.0/*.jpg'))\n",
    "    ID_CE_list = sorted(glob.glob(path_ID+'/1.0CE/*.jpg'))\n",
    "\n",
    "    if CT_type == 'P':\n",
    "        ID_X_list = ID_P_list\n",
    "    if CT_type == 'CE':\n",
    "        ID_X_list = ID_CE_list\n",
    "\n",
    "    for j in range(len(ID_X_list)-inchan+1):                    \n",
    "        trans_list = list()\n",
    "        for k in range(inchan):\n",
    "            trans_list.append(ID_X_list[j+k])\n",
    "        train_imgs_list.append(trans_list)\n",
    "\n",
    "for ID in In_Val_ID_list:\n",
    "    path_ID = os.path.join(path_In_Val,ID)\n",
    "    ID_P_list  = sorted(glob.glob(path_ID+'/1.0/*.jpg'))\n",
    "    ID_CE_list = sorted(glob.glob(path_ID+'/1.0CE/*.jpg'))\n",
    "\n",
    "    if CT_type == 'P':\n",
    "        ID_X_list = ID_P_list\n",
    "    if CT_type == 'CE':\n",
    "        ID_X_list = ID_CE_list\n",
    "\n",
    "    for j in range(len(ID_X_list)-inchan+1):                    \n",
    "        trans_list = list()\n",
    "        for k in range(inchan):\n",
    "            trans_list.append(ID_X_list[j+k])\n",
    "        test_imgs_list.append(trans_list)            \n",
    "\n",
    "random.shuffle(train_imgs_list)\n",
    "random.shuffle(test_imgs_list)\n",
    "\n",
    "train_ds1 = CTdataset(train_imgs_list,transform, size_in)\n",
    "train_ds2 = CTdataset(train_imgs_list,mirror, size_in)\n",
    "train_ds = train_ds1 + train_ds2\n",
    "test_ds = CTdataset(test_imgs_list,transform, size_in)\n",
    "\n",
    "N = len(train_ds)\n",
    "weights = [4.0 if label == 0  else 1 for data, label in train_ds] # RCC_patients/fp_AML_patients â‰ˆ  4\n",
    "sampler = WeightedRandomSampler(weights,num_samples= N ,replacement=True)\n",
    "train_dl = get_data(train_ds, bs, sampler)\n",
    "test_dl = get_data(test_ds, bs2)\n",
    "\n",
    "model = xception(n_class, inchan, 0.2) # dropout = 0.2\n",
    "model.to(dev)\n",
    "\n",
    "opt = optim.Adam(model.parameters()) #default_lr = 0.001\n",
    "\n",
    "epoch_n = list()\n",
    "loss_training = list()\n",
    "loss_testing = list()\n",
    "acc_training = list()\n",
    "acc_testing = list()\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, test_dl)\n",
    "\n",
    "df_fore = {'epoch_n':epoch_n,\n",
    "           'loss_training':loss_training,\n",
    "           'loss_testing':loss_testing,\n",
    "           'acc_training':acc_training,\n",
    "           'acc_testing':acc_testing}\n",
    "\n",
    "df = pd.DataFrame(df_fore)\n",
    "df.to_csv(csv_name_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae42fb5-03fb-4a3a-974d-8efee16299cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T08:09:13.166804Z",
     "iopub.status.busy": "2023-01-15T08:09:13.166292Z",
     "iopub.status.idle": "2023-01-15T08:09:13.174741Z",
     "shell.execute_reply": "2023-01-15T08:09:13.173498Z",
     "shell.execute_reply.started": "2023-01-15T08:09:13.166742Z"
    }
   },
   "source": [
    "# Internal_validation_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618d1c48-4fa8-4ec6-b913-16f781963d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val = xception(n_class,inchan,0)\n",
    "model_val.load_state_dict(t.load(best_acc_name))\n",
    "model_val.to(dev)\n",
    "\n",
    "model_val.eval()\n",
    "with t.no_grad():\n",
    "    acc_for_imgs = list()\n",
    "    labels_test = list()\n",
    "    preds_list = list()\n",
    "    for ID in In_Val_ID_list:\n",
    "        score_pic = list()\n",
    "        ID_root,label = get_ID_root_label(ID,path_In_Val)\n",
    "        for root in ID_root:\n",
    "            image_data = np.zeros([size_in, size_in,inchan])\n",
    "            for j in range(inchan):\n",
    "                pil_img_n = Image.open(root[j]).convert('L').resize((size_in, size_in),Image.ANTIALIAS)\n",
    "                image_data[:,:,j] = pil_img_n\n",
    "            image_data = image_data.astype(np.float32)\n",
    "            img = transform(image_data).unsqueeze(0)\n",
    "            img = img.to(dev)\n",
    "            out= model_val(img)\n",
    "            pred = t.argmax(out,dim=1)\n",
    "            pred = pred.item()\n",
    "            score_pic.append(pred)\n",
    "            if pred == int(label):\n",
    "                acc_for_imgs.append(1)\n",
    "            else:\n",
    "                acc_for_imgs.append(0)\n",
    "\n",
    "        score_ID = np.array(score_pic).mean()\n",
    "        labels_test.append(int(label))\n",
    "        preds_list.append(score_ID)\n",
    "        \n",
    "    acc_img_level_testing = np.array(acc_for_imgs).mean()\n",
    "    fpr, tpr, thresholds = roc_curve(labels_test, preds_list)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    optimal_th, optimal_point = Find_Optimal_Cutoff(TPR=tpr, FPR=fpr, Thresholds=thresholds)\n",
    "    \n",
    "    print('In_Val_Image_level_ACC:',acc_img_level_testing)\n",
    "    print('In_Val_Patient_level_AUC:',roc_auc,', fpr:',optimal_point[0], ', tpr:',optimal_point[1],', cut-off:',optimal_th)\n",
    "\n",
    "    with open(file_record , \"a\") as f:\n",
    "        f.write('In_Val_Image_level_ACC:'+str(acc_img_level_testing)+'\\n')\n",
    "        f.write('In_Val_Patient_level_AUC:'+str(roc_auc)+', fpr:'+str(optimal_point[0])+', tpr:'+str(optimal_point[1])+', cut-off:'+str(optimal_th)+'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd68279-c264-427a-bd60-82cd14174752",
   "metadata": {},
   "source": [
    "# External_validation_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d94c81-cb81-4533-9706-a22a2f7d8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val = xception(n_class,inchan,0)\n",
    "model_val.load_state_dict(t.load(best_acc_name))\n",
    "model_val.to(dev)\n",
    "\n",
    "model_val.eval()\n",
    "with t.no_grad():\n",
    "    acc_for_imgs = list()\n",
    "    labels_test = list()\n",
    "    preds_list = list()\n",
    "    for ID in Ex_Val_ID_list:\n",
    "        score_pic = list()\n",
    "        ID_root,label = get_ID_root_label(ID,path_Ex_Val)\n",
    "        for root in ID_root:\n",
    "            image_data = np.zeros([size_in, size_in,inchan])\n",
    "            for j in range(inchan):\n",
    "                pil_img_n = Image.open(root[j]).convert('L').resize((size_in, size_in),Image.ANTIALIAS)\n",
    "                image_data[:,:,j] = pil_img_n\n",
    "            image_data = image_data.astype(np.float32)\n",
    "            img = transform(image_data).unsqueeze(0)\n",
    "            img = img.to(dev)\n",
    "            out= model_val(img)\n",
    "            pred = t.argmax(out,dim=1)\n",
    "            pred = pred.item()\n",
    "            score_pic.append(pred)\n",
    "            if pred == int(label):\n",
    "                acc_for_imgs.append(1)\n",
    "            else:\n",
    "                acc_for_imgs.append(0)\n",
    "\n",
    "        score_ID = np.array(score_pic).mean()\n",
    "        labels_test.append(int(label))\n",
    "        preds_list.append(score_ID)\n",
    "        \n",
    "    acc_img_level_testing = np.array(acc_for_imgs).mean()\n",
    "    fpr, tpr, thresholds = roc_curve(labels_test, preds_list)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    optimal_th, optimal_point = Find_Optimal_Cutoff(TPR=tpr, FPR=fpr, Thresholds=thresholds)\n",
    "    \n",
    "    print('Ex_Val_Image_level_ACC:',acc_img_level_testing)\n",
    "    print('Ex_Val_Patient_level_AUC:',roc_auc,', fpr:',optimal_point[0], ', tpr:',optimal_point[1],', cut-off:',optimal_th)\n",
    "\n",
    "    with open(file_record , \"a\") as f:\n",
    "        f.write('Ex_Val_Image_level_ACC:'+str(acc_img_level_testing)+'\\n')\n",
    "        f.write('Ex_Val_Patient_level_AUC:'+str(roc_auc)+', fpr:'+str(optimal_point[0])+', tpr:'+str(optimal_point[1])+', cut-off:'+str(optimal_th)+'\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
