{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40cc4f16-ffc3-4e04-88c5-12819e943ff6",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2bd40d0-c6c4-4785-a91b-3ad56fd97767",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T12:48:07.424928Z",
     "iopub.status.busy": "2023-01-15T12:48:07.424361Z",
     "iopub.status.idle": "2023-01-15T12:48:07.433466Z",
     "shell.execute_reply": "2023-01-15T12:48:07.432568Z",
     "shell.execute_reply.started": "2023-01-15T12:48:07.424892Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch import optim, nn\n",
    "from PIL import  Image\n",
    "from torchvision import transforms as T\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import  WeightedRandomSampler\n",
    "import random\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18364da-2951-4510-8dcc-cb2b99b71235",
   "metadata": {},
   "source": [
    "# set_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e720ac-dd3f-414a-a3fd-cb45abc8c03e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T12:12:34.233051Z",
     "iopub.status.busy": "2023-01-15T12:12:34.232782Z",
     "iopub.status.idle": "2023-01-15T12:12:34.250232Z",
     "shell.execute_reply": "2023-01-15T12:12:34.249522Z",
     "shell.execute_reply.started": "2023-01-15T12:12:34.233020Z"
    }
   },
   "outputs": [],
   "source": [
    "inchan = 1 # set 1 to 10\n",
    "CT_type = 'P' # 'P' or 'CE'\n",
    "seed = 1 # random seed\n",
    "gpu = \"cuda:1\"\n",
    "n_class = 2 # classes number\n",
    "save_num = 0 # 5-fold cross-validation from 0 to 4\n",
    "save_state = 1 # save the training state 0 or 1\n",
    "\n",
    "model_name = 'Xception'\n",
    "opt_name = 'adam'\n",
    "epochs = 50 # epoch number\n",
    "size_in = 171\n",
    "bs  = 120 # training_batchsize\n",
    "bs2 = 60 # testing_batchsize\n",
    "\n",
    "pre_dir_name = ('CV_save/save_'+str(save_num)+'/'\n",
    "                +'CT_type:'+str(CT_type)\n",
    "                +'-inchan:'+str(inchan)\n",
    "                +'-n_classes:'+str(n_class)\n",
    "                +'-seed:'+str(seed))\n",
    "\n",
    "path_CV = 'data/Center1'\n",
    "\n",
    "df_sample_list = pd.read_csv('CV_sample_list.csv')\n",
    "train_ID_list = df_sample_list[(df_sample_list['fold_num'] == save_num)&(df_sample_list['train'] == 1)]['ID'].tolist()\n",
    "test_ID_list = df_sample_list[(df_sample_list['fold_num'] == save_num)&(df_sample_list['train'] == 0)]['ID'].tolist()\n",
    "\n",
    "dev = t.device(gpu) if t.cuda.is_available() else t.device(\"cpu\")\n",
    "\n",
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac667d0b-0b46-48d6-9a18-9f05194d4045",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1650f088-17ff-49f1-ab87-923640a2a05d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T12:12:38.734131Z",
     "iopub.status.busy": "2023-01-15T12:12:38.733833Z",
     "iopub.status.idle": "2023-01-15T12:12:38.757389Z",
     "shell.execute_reply": "2023-01-15T12:12:38.756606Z",
     "shell.execute_reply.started": "2023-01-15T12:12:38.734100Z"
    }
   },
   "outputs": [],
   "source": [
    "if inchan == 1 : \n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(), \n",
    "        T.Normalize(mean=[.5], std=[.5]) \n",
    "    ])\n",
    "\n",
    "    mirror = T.Compose([\n",
    "        T.ToTensor(), \n",
    "        T.Normalize(mean=[.5], std=[.5]), \n",
    "        T.RandomHorizontalFlip(p=1)\n",
    "    ])\n",
    "\n",
    "    class CTdataset(Dataset):\n",
    "        def __init__(self, imgs_list,transform = None, size_in = None):\n",
    "            self.imgs_path = imgs_list\n",
    "            self.transform = transform\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            img_path = self.imgs_path[index]\n",
    "            if 'KF' in img_path:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "\n",
    "            pil_img = Image.open(img_path).convert('L').resize((size_in, size_in),Image.ANTIALIAS)\n",
    "            image_data = np.zeros([size_in, size_in,inchan])\n",
    "            image_data[:,:,0] = pil_img\n",
    "            image_data = image_data.astype(np.float32)\n",
    "\n",
    "            if self.transform:\n",
    "                data = self.transform(image_data)\n",
    "            else:\n",
    "                data = image_data\n",
    "            return data, label\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.imgs_path)\n",
    "\n",
    "if inchan >= 2 : # 1.0 or 1.0CE\n",
    "    std_list = list()\n",
    "    for i in range(inchan):\n",
    "        std_list.append(0.5)\n",
    "    transform = T.Compose([\n",
    "        T.ToTensor(), \n",
    "        T.Normalize(mean=std_list, std=std_list) \n",
    "    ])\n",
    "\n",
    "    mirror = T.Compose([\n",
    "        T.ToTensor(), \n",
    "        T.Normalize(mean=std_list, std=std_list), \n",
    "        T.RandomHorizontalFlip(p=1)\n",
    "    ])\n",
    "\n",
    "\n",
    "    class CTdataset(Dataset):\n",
    "        def __init__(self, imgs_list,transform = None, size_in = None):\n",
    "            self.imgs_path = imgs_list\n",
    "            self.transform = transform\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            img_nX = self.imgs_path[index]\n",
    "            img_X = img_nX[0]\n",
    "            if 'KF' in img_X:\n",
    "                label = 0\n",
    "            else:\n",
    "                label = 1\n",
    "\n",
    "            image_data = np.zeros([size_in, size_in,inchan])\n",
    "            for i in range(inchan):\n",
    "                pil_img_n = Image.open(img_nX[i]).convert('L').resize((size_in, size_in),Image.ANTIALIAS)\n",
    "                image_data[:,:,i] = pil_img_n\n",
    "            image_data = image_data.astype(np.float32)\n",
    "\n",
    "            if self.transform:\n",
    "                data = self.transform(image_data)\n",
    "            else:\n",
    "                data = image_data\n",
    "            return data, label\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.imgs_path)\n",
    "\n",
    "def loss_batch(model, loss_func, xb, yb, opt=None):\n",
    "    out = model(xb)\n",
    "    preds = t.argmax(out,dim=1)\n",
    "    loss = loss_func(out, yb)\n",
    "    if opt is not None:\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return loss.item(), preds\n",
    "\n",
    "\n",
    "def get_data(train_ds,bs,sampler=None):\n",
    "    if sampler:\n",
    "        t1 = DataLoader(train_ds, batch_size=bs,sampler=sampler)\n",
    "    else:\n",
    "        t1 = DataLoader(train_ds, batch_size=bs)\n",
    "    return t1\n",
    "\n",
    "def get_ID_root_label(ID,path):\n",
    "    ID_root_list = list()\n",
    "                   \n",
    "    if inchan >= 2:\n",
    "        path_ID = os.path.join(path,ID)\n",
    "        ID_P_list = sorted(glob.glob(path_ID+'/1.0/*.jpg'))\n",
    "        ID_CE_list = sorted(glob.glob(path_ID+'/1.0CE/*.jpg'))\n",
    "        if CT_type == 'P':\n",
    "            ID_X_list = ID_P_list\n",
    "        if CT_type == 'CE':\n",
    "            ID_X_list = ID_CE_list\n",
    "\n",
    "        for j in range(len(ID_X_list)-inchan+1):                    \n",
    "            trans_list = list()\n",
    "            for k in range(inchan):\n",
    "                trans_list.append(ID_X_list[j+k])\n",
    "            ID_root_list.append(trans_list)\n",
    "\n",
    "    if inchan == 1:\n",
    "        path_ID = os.path.join(path,ID)\n",
    "        ID_P_list = sorted(glob.glob(path_ID+'/1.0/*.jpg'))\n",
    "        ID_CE_list = sorted(glob.glob(path_ID+'/1.0CE/*.jpg'))       \n",
    "        if CT_type == 'P':\n",
    "            ID_root_list = ID_P_list\n",
    "            ID_X_list = ID_P_list\n",
    "        if CT_type == 'CE':\n",
    "            ID_root_list = ID_CE_list\n",
    "            ID_X_list = ID_CE_list\n",
    "    \n",
    "    if 'KF' in ID_X_list[0]:\n",
    "        label = 0\n",
    "    else:\n",
    "        label = 1\n",
    "    return(ID_root_list, label)\n",
    "\n",
    "def Find_Optimal_Cutoff(TPR, FPR, Thresholds):\n",
    "    y = TPR - FPR\n",
    "    Youden_index = np.argmax(y) \n",
    "    optimal_threshold = Thresholds[Youden_index]\n",
    "    point = [FPR[Youden_index], TPR[Youden_index]]\n",
    "    return optimal_threshold, point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ac7e11-5b1f-421d-9a42-431526a56fe3",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "346be9e7-e065-4293-a858-9d582c27729a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T12:12:40.210094Z",
     "iopub.status.busy": "2023-01-15T12:12:40.209762Z",
     "iopub.status.idle": "2023-01-15T12:12:40.233107Z",
     "shell.execute_reply": "2023-01-15T12:12:40.232313Z",
     "shell.execute_reply.started": "2023-01-15T12:12:40.210060Z"
    }
   },
   "outputs": [],
   "source": [
    "class depthwise_separable_conv(nn.Module):\n",
    "    def __init__(self, nin, nout, kernel_size, padding, bias=False):\n",
    "        super(depthwise_separable_conv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(nin, nin, kernel_size=kernel_size, padding=padding, groups=nin, bias=bias)\n",
    "        self.pointwise = nn.Conv2d(nin, nout, kernel_size=1, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.depthwise(x)\n",
    "        out = self.pointwise(out)                                               \n",
    "        return out\n",
    "    \n",
    "class Xception(nn.Module):\n",
    "    def __init__(self,num_classes= None ,inchan = None, p_drop = None):\n",
    "        super(Xception, self).__init__()\n",
    "        \n",
    "        self.p_drop = p_drop\n",
    "        \n",
    "        # Entry Flow\n",
    "        self.entry_flow_1 = nn.Sequential(\n",
    "            nn.Conv2d(inchan, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.entry_flow_2 = nn.Sequential(\n",
    "            depthwise_separable_conv(64, 128, 3, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            depthwise_separable_conv(128, 128, 3, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.entry_flow_2_residual = nn.Conv2d(64, 128, kernel_size=1, stride=2, padding=0)\n",
    "        \n",
    "        self.entry_flow_3 = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(128, 256, 3, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(256, 256, 3, 1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.entry_flow_3_residual = nn.Conv2d(128, 256, kernel_size=1, stride=2, padding=0)\n",
    "        \n",
    "        self.entry_flow_4 = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(256, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        \n",
    "        self.entry_flow_4_residual = nn.Conv2d(256, 728, kernel_size=1, stride=2, padding=0)\n",
    "        \n",
    "        # Middle Flow\n",
    "        self.middle_flow = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728)\n",
    "        )\n",
    "        \n",
    "        # Exit Flow\n",
    "        self.exit_flow_1 = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 728, 3, 1),\n",
    "            nn.BatchNorm2d(728),\n",
    "            \n",
    "            nn.ReLU(True),\n",
    "            depthwise_separable_conv(728, 1024, 3, 1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "        self.exit_flow_1_residual = nn.Conv2d(728, 1024, kernel_size=1, stride=2, padding=0)\n",
    "        self.exit_flow_2 = nn.Sequential(\n",
    "            depthwise_separable_conv(1024, 1536, 3, 1),\n",
    "            nn.BatchNorm2d(1536),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            depthwise_separable_conv(1536, 2048, 3, 1),\n",
    "            nn.BatchNorm2d(2048),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(2048, 1024)\n",
    "        self.linear2 = nn.Linear(1024, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        entry_out1 = self.entry_flow_1(x)\n",
    "        entry_out2 = self.entry_flow_2(entry_out1) + self.entry_flow_2_residual(entry_out1)\n",
    "        entry_out3 = self.entry_flow_3(entry_out2) + self.entry_flow_3_residual(entry_out2)\n",
    "        entry_out = self.entry_flow_4(entry_out3) + self.entry_flow_4_residual(entry_out3)\n",
    "        \n",
    "        middle_out = self.middle_flow(entry_out) + entry_out\n",
    "\n",
    "        exit_out1 = self.exit_flow_1(middle_out) + self.exit_flow_1_residual(middle_out)\n",
    "        exit_out2 = self.exit_flow_2(exit_out1)\n",
    "\n",
    "        exit_avg_pool = F.adaptive_avg_pool2d(exit_out2, (1, 1))     \n",
    "        exit_avg_pool_flat = exit_avg_pool.view(exit_avg_pool.size(0), -1)\n",
    "\n",
    "        exit_avg_pool_flat = nn.Dropout(p=self.p_drop)(exit_avg_pool_flat)\n",
    "        output = self.linear1(exit_avg_pool_flat)\n",
    "        output = self.linear2(output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "def xception(x1,x2,x3):\n",
    "    return Xception(num_classes= x1 ,inchan = x2, p_drop = x3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5957069-c42c-4d49-9786-316a4917d713",
   "metadata": {},
   "source": [
    "# fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff760e71-fc10-40ef-8d4f-ab3ca94f9b52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-15T12:12:41.554287Z",
     "iopub.status.busy": "2023-01-15T12:12:41.553982Z",
     "iopub.status.idle": "2023-01-15T12:12:41.568596Z",
     "shell.execute_reply": "2023-01-15T12:12:41.567833Z",
     "shell.execute_reply.started": "2023-01-15T12:12:41.554256Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit(epochs, model, loss_func, opt, train_dl, test_dl):\n",
    "    best_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_n.append(epoch)\n",
    "        \n",
    "        model.train()\n",
    "        train_acc = list()\n",
    "        train_loss = list()\n",
    "        for xb, yb in train_dl:\n",
    "            xb = xb.to(dev)\n",
    "            yb = yb.to(dev)\n",
    "            tbs_loss, tbs_preds = loss_batch(model, loss_func, xb, yb, opt)\n",
    "            tbs_acc = (tbs_preds == yb).float().mean()\n",
    "            train_loss.append(tbs_loss)\n",
    "            train_acc.append(tbs_acc.tolist())\n",
    "        tr_loss= np.array(train_loss).mean()\n",
    "        tr_acc = np.array(train_acc).mean()\n",
    "        \n",
    "        print('epoch:', epoch, \", trian_loss:\",tr_loss,', trian_acc:',tr_acc )\n",
    "        with open(file_record , \"a\") as f:\n",
    "            f.write('epoch:'+str(epoch)+\", trian_loss:\"+str(tr_loss)+', trian_acc:'+str(tr_acc)+'\\n')\n",
    "        \n",
    "        loss_training.append(tr_loss)\n",
    "        acc_training.append(tr_acc)\n",
    "        \n",
    "        if save_state == 1:\n",
    "            t.save(model.state_dict(), state_name)\n",
    "        \n",
    "        model.eval()\n",
    "        with t.no_grad(): \n",
    "            test_acc = list()\n",
    "            test_loss = list()\n",
    "            for xb, yb in test_dl:\n",
    "                xb = xb.to(dev)\n",
    "                yb = yb.to(dev)\n",
    "                tebs_loss, tebs_preds = loss_batch(model, loss_func, xb, yb)\n",
    "                tebs_acc = (tebs_preds == yb).float().mean()\n",
    "                test_loss.append(tebs_loss)\n",
    "                test_acc.append(tebs_acc.tolist())\n",
    "            \n",
    "            te_loss = np.array(test_loss).mean()\n",
    "            te_acc = np.array(test_acc).mean()\n",
    "            \n",
    "            now_acc = te_acc\n",
    "            \n",
    "            print(\"test_loss:\", te_loss ,\", test_acc:\", now_acc )\n",
    "            with open(file_record , \"a\") as f:\n",
    "                f.write(\"test_loss:\"+str(te_loss)+\", test_acc:\"+str(now_acc)+'\\n' )\n",
    "            \n",
    "            loss_testing.append(te_loss)\n",
    "            acc_testing.append(te_acc)\n",
    "            \n",
    "            if epoch == 0 and save_state == 1: \n",
    "                if save_state == 1:\n",
    "                    t.save(model.state_dict(), best_acc_name)\n",
    "                best_acc = now_acc\n",
    "                best_acc_epoch = epoch\n",
    "            if now_acc > best_acc and epoch > 0 and save_state == 1:\n",
    "                if save_state == 1:\n",
    "                    t.save(model.state_dict(), best_acc_name)\n",
    "                best_acc = now_acc\n",
    "                best_acc_epoch = epoch\n",
    "\n",
    "    print('best_acc_testing:',max(acc_testing),', best_epoch:',best_acc_epoch)\n",
    "    \n",
    "    with open(file_record , \"a\") as f:\n",
    "        f.write('best_epoch:'+str(best_acc_epoch)+'\\n'+'best_acc_testing:'+str(max(acc_testing))+'\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07be4c18-a2f9-4049-b870-fea670e7127c",
   "metadata": {},
   "source": [
    "# start_fold_traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d03ddee-8fc0-460a-9bc9-39cc8f10401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = (pre_dir_name + '/' + model_name +'-'+ opt_name)          \n",
    "file_record = path_save+'/record-{0}-{1}.txt'.format(model_name, opt_name)\n",
    "csv_name_path = path_save+ '/record-{0}-{1}-lines.csv'.format(model_name, opt_name)\n",
    "state_name = path_save+ '/'+'state-{0}-{1}-done.pkl'.format(model_name, opt_name)\n",
    "best_acc_name = path_save+ '/'+'best_acc-{0}-{1}-done.pkl'.format(model_name, opt_name)\n",
    "\n",
    "print('path_save:',path_save)\n",
    "print('file_record:',file_record)\n",
    "print('csv_name_path:',csv_name_path)\n",
    "\n",
    "if save_state == 1:\n",
    "    if not os.path.exists(path_save):\n",
    "        os.makedirs(path_save)\n",
    "    else:\n",
    "        print('exist')\n",
    "\n",
    "train_imgs_list = list()\n",
    "test_imgs_list = list()\n",
    "\n",
    "if inchan >= 2:\n",
    "    for ID in train_ID_list:\n",
    "        path_ID = os.path.join(path_CV,ID)\n",
    "        ID_P_list = sorted(glob.glob(path_ID+'/1.0/*.jpg'))\n",
    "        ID_CE_list = sorted(glob.glob(path_ID+'/1.0CE/*.jpg'))\n",
    "\n",
    "        if CT_type == 'P':\n",
    "            ID_X_list = ID_P_list\n",
    "        if CT_type == 'CE':\n",
    "            ID_X_list = ID_CE_list\n",
    "\n",
    "        for j in range(len(ID_X_list)-inchan+1):                    \n",
    "            trans_list = list()\n",
    "            for k in range(inchan):\n",
    "                trans_list.append(ID_X_list[j+k])\n",
    "            train_imgs_list.append(trans_list)\n",
    "\n",
    "    for ID in test_ID_list:\n",
    "        path_ID = os.path.join(path_CV,ID)\n",
    "        ID_P_list  = sorted(glob.glob(path_ID+'/1.0/*.jpg'))\n",
    "        ID_CE_list = sorted(glob.glob(path_ID+'/1.0CE/*.jpg'))\n",
    "        \n",
    "        if CT_type == 'P':\n",
    "            ID_X_list = ID_P_list\n",
    "        if CT_type == 'CE':\n",
    "            ID_X_list = ID_CE_list\n",
    "\n",
    "        for j in range(len(ID_X_list)-inchan+1):                    \n",
    "            trans_list = list()\n",
    "            for k in range(inchan):\n",
    "                trans_list.append(ID_X_list[j+k])\n",
    "            test_imgs_list.append(trans_list)    \n",
    "\n",
    "if inchan == 1:\n",
    "    for ID in train_ID_list:\n",
    "        path_ID = os.path.join(path_CV,ID)\n",
    "\n",
    "        ID_P_list = sorted(glob.glob(path_ID+'/1.0/*.jpg'))\n",
    "        ID_CE_list = sorted(glob.glob(path_ID+'/1.0CE/*.jpg'))\n",
    "\n",
    "        if CT_type == 'P':\n",
    "            train_imgs_list = train_imgs_list + ID_P_list\n",
    "        if CT_type == 'CE':\n",
    "            train_imgs_list = train_imgs_list + ID_CE_list\n",
    "\n",
    "    for ID in test_ID_list:\n",
    "        path_ID = os.path.join(path_CV,ID)\n",
    "\n",
    "        ID_P_list = sorted(glob.glob(path_ID+'/1.0/*.jpg'))\n",
    "        ID_CE_list = sorted(glob.glob(path_ID+'/1.0CE/*.jpg'))\n",
    "\n",
    "        if CT_type == 'P':\n",
    "            test_imgs_list = test_imgs_list + ID_P_list\n",
    "        if CT_type == 'CE':\n",
    "            test_imgs_list = test_imgs_list + ID_CE_list         \n",
    "\n",
    "random.shuffle(train_imgs_list)\n",
    "random.shuffle(test_imgs_list)\n",
    "\n",
    "train_ds1 = CTdataset(train_imgs_list,transform, size_in)\n",
    "train_ds2 = CTdataset(train_imgs_list,mirror, size_in)\n",
    "train_ds = train_ds1 + train_ds2\n",
    "test_ds = CTdataset(test_imgs_list,transform, size_in)\n",
    "\n",
    "N = len(train_ds)\n",
    "weights = [4.0 if label == 0  else 1 for data, label in train_ds] # RCC_patients/fp_AML_patients â‰ˆ  4\n",
    "sampler = WeightedRandomSampler(weights,num_samples= N ,replacement=True)\n",
    "train_dl = get_data(train_ds, bs, sampler)\n",
    "test_dl = get_data(test_ds, bs2)\n",
    "\n",
    "model = xception(n_class, inchan, 0.2) # dropout = 0.2\n",
    "model.to(dev)\n",
    "\n",
    "opt = optim.Adam(model.parameters()) #default_lr = 0.001\n",
    "\n",
    "epoch_n = list()\n",
    "loss_training = list()\n",
    "loss_testing = list()\n",
    "acc_training = list()\n",
    "acc_testing = list()\n",
    "\n",
    "fit(epochs, model, loss_func, opt, train_dl, test_dl)\n",
    "\n",
    "df_fore = {'epoch_n':epoch_n,\n",
    "           'loss_training':loss_training,\n",
    "           'loss_testing':loss_testing,\n",
    "           'acc_training':acc_training,\n",
    "           'acc_testing':acc_testing}\n",
    "\n",
    "df = pd.DataFrame(df_fore)\n",
    "df.to_csv(csv_name_path, header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef41a068-4286-456a-9063-c220f0f982b2",
   "metadata": {},
   "source": [
    "# fold_best_testing_AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6287c44-450f-48ec-a074-cd4954c68429",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_val = xception(n_class,inchan,0)\n",
    "model_val.load_state_dict(t.load(best_acc_name))\n",
    "model_val.to(dev)\n",
    "\n",
    "model_val.eval()\n",
    "with t.no_grad():\n",
    "    acc_for_imgs = list()\n",
    "    labels_test = list()\n",
    "    preds_list = list()\n",
    "    for ID in test_ID_list:\n",
    "        score_pic = list()\n",
    "        ID_root,label = get_ID_root_label(ID,path_CV)\n",
    "        if inchan >= 2:\n",
    "            for root in ID_root:\n",
    "                image_data = np.zeros([size_in, size_in,inchan])\n",
    "                for j in range(inchan):\n",
    "                    pil_img_n = Image.open(root[j]).convert('L').resize((size_in, size_in),Image.ANTIALIAS)\n",
    "                    image_data[:,:,j] = pil_img_n\n",
    "                image_data = image_data.astype(np.float32)\n",
    "                img = transform(image_data).unsqueeze(0)\n",
    "                img = img.to(dev)\n",
    "                out= model_val(img)\n",
    "                pred = t.argmax(out,dim=1)\n",
    "                pred = pred.item()\n",
    "                score_pic.append(pred)\n",
    "                if pred == int(label):\n",
    "                    acc_for_imgs.append(1)\n",
    "                else:\n",
    "                    acc_for_imgs.append(0)\n",
    "\n",
    "        if inchan == 1:\n",
    "            for root in ID_root:\n",
    "                pil_img_P = Image.open(root).convert('L').resize((size_in, size_in),Image.ANTIALIAS)\n",
    "                image_data = np.zeros([size_in, size_in,1])\n",
    "                image_data[:,:,0] = pil_img_P\n",
    "                image_data = image_data.astype(np.float32)\n",
    "                img = transform(image_data).unsqueeze(0)\n",
    "                img = img.to(dev)\n",
    "                out= model_val(img)\n",
    "                pred = t.argmax(out,dim=1)\n",
    "                pred = pred.item()\n",
    "                score_pic.append(pred)\n",
    "                if pred == int(label):\n",
    "                    acc_for_imgs.append(1)\n",
    "                else:\n",
    "                    acc_for_imgs.append(0)\n",
    "        score_ID = np.array(score_pic).mean()\n",
    "        labels_test.append(int(label))\n",
    "        preds_list.append(score_ID)\n",
    "        \n",
    "    acc_img_level_testing = np.array(acc_for_imgs).mean()\n",
    "    fpr, tpr, thresholds = roc_curve(labels_test, preds_list)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    optimal_th, optimal_point = Find_Optimal_Cutoff(TPR=tpr, FPR=fpr, Thresholds=thresholds)\n",
    "    \n",
    "    print('Image_level_ACC:',acc_img_level_testing)\n",
    "    print('Patient_level_AUC:',roc_auc,', fpr:',optimal_point[0], ', tpr:',optimal_point[1],', cut-off:',optimal_th)\n",
    "\n",
    "    with open(file_record , \"a\") as f:\n",
    "        f.write('Image_level_ACC:'+str(acc_img_level_testing)+'\\n')\n",
    "        f.write('Patient_level_AUC:'+str(roc_auc)+', fpr:'+str(optimal_point[0])+', tpr:'+str(optimal_point[1])+', cut-off:'+str(optimal_th)+'\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python37",
   "language": "python",
   "name": "python37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
